# Complete observability stack for MicroShift edge deployments
# This includes OpenTelemetry Collector, Jaeger for tracing, and basic monitoring

---
apiVersion: v1
kind: Namespace
metadata:
  name: observability
  labels:
    name: observability

---
# OpenTelemetry Collector ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
  namespace: observability
data:
  config.yaml: |
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
      
      # Kubernetes cluster metrics
      k8s_cluster:
        auth_type: serviceAccount
        node: ${NODE_NAME}
        
      # Kubelet metrics
      kubeletstats:
        collection_interval: 30s
        auth_type: serviceAccount
        endpoint: https://${NODE_IP}:10250
        insecure_skip_verify: true
        
    processors:
      resource:
        attributes:
          - key: cluster.name
            value: "microshift-edge"
            action: upsert
          - key: deployment.environment
            value: "edge"
            action: upsert
            
      batch:
        timeout: 10s
        send_batch_size: 1024
        
      memory_limiter:
        limit_mib: 256
        spike_limit_mib: 64
        
    exporters:
      # Jaeger exporter for traces
      jaeger:
        endpoint: jaeger-collector.observability.svc.cluster.local:14250
        tls:
          insecure: true
          
      # Prometheus exporter for metrics
      prometheus:
        endpoint: "0.0.0.0:9464"
        namespace: "microshift"
        
      # Logging for debugging
      logging:
        loglevel: info
        
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, resource, batch]
          exporters: [jaeger, logging]
          
        metrics:
          receivers: [otlp, k8s_cluster, kubeletstats]
          processors: [memory_limiter, resource, batch]
          exporters: [prometheus]
          
        logs:
          receivers: [otlp]
          processors: [memory_limiter, resource, batch]
          exporters: [logging]
          
      telemetry:
        logs:
          level: "info"
        metrics:
          address: "0.0.0.0:8888"

---
# OpenTelemetry Collector Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-collector
  namespace: observability
  labels:
    app: otel-collector
spec:
  replicas: 1
  selector:
    matchLabels:
      app: otel-collector
  template:
    metadata:
      labels:
        app: otel-collector
    spec:
      serviceAccountName: otel-collector
      containers:
      - name: otel-collector
        image: otel/opentelemetry-collector-contrib:latest
        command:
          - "/otelcol-contrib"
          - "--config=/etc/otel-collector-config/config.yaml"
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: NODE_IP
          valueFrom:
            fieldRef:
              fieldPath: status.hostIP
        ports:
        - containerPort: 4317   # OTLP gRPC
        - containerPort: 4318   # OTLP HTTP
        - containerPort: 9464   # Prometheus metrics
        - containerPort: 8888   # Internal metrics
        volumeMounts:
        - name: config
          mountPath: /etc/otel-collector-config
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
      volumes:
      - name: config
        configMap:
          name: otel-collector-config

---
# OpenTelemetry Collector Service
apiVersion: v1
kind: Service
metadata:
  name: otel-collector
  namespace: observability
  labels:
    app: otel-collector
spec:
  type: NodePort
  ports:
  - name: otlp-grpc
    port: 4317
    targetPort: 4317
    nodePort: 30317
  - name: otlp-http
    port: 4318
    targetPort: 4318
    nodePort: 30318
  - name: prometheus
    port: 9464
    targetPort: 9464
    nodePort: 30464
  selector:
    app: otel-collector

---
# ServiceAccount for OpenTelemetry Collector
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otel-collector
  namespace: observability

---
# ClusterRole for OpenTelemetry Collector
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: otel-collector
rules:
- apiGroups: [""]
  resources: ["nodes", "nodes/proxy", "nodes/metrics", "services", "endpoints", "pods"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["get"]
- nonResourceURLs: ["/metrics", "/metrics/cadvisor"]
  verbs: ["get"]

---
# ClusterRoleBinding for OpenTelemetry Collector
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: otel-collector
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: otel-collector
subjects:
- kind: ServiceAccount
  name: otel-collector
  namespace: observability

---
# Jaeger All-in-One Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jaeger
  namespace: observability
  labels:
    app: jaeger
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jaeger
  template:
    metadata:
      labels:
        app: jaeger
    spec:
      containers:
      - name: jaeger
        image: jaegertracing/all-in-one:latest
        env:
        - name: COLLECTOR_ZIPKIN_HOST_PORT
          value: ":9411"
        - name: COLLECTOR_OTLP_ENABLED
          value: "true"
        ports:
        - containerPort: 16686  # Jaeger UI
        - containerPort: 14250  # gRPC
        - containerPort: 14268  # HTTP
        - containerPort: 9411   # Zipkin
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"

---
# Jaeger Service
apiVersion: v1
kind: Service
metadata:
  name: jaeger-collector
  namespace: observability
  labels:
    app: jaeger
spec:
  ports:
  - name: grpc
    port: 14250
    targetPort: 14250
  - name: http
    port: 14268
    targetPort: 14268
  - name: zipkin
    port: 9411
    targetPort: 9411
  selector:
    app: jaeger

---
# Jaeger UI Service
apiVersion: v1
kind: Service
metadata:
  name: jaeger-ui
  namespace: observability
  labels:
    app: jaeger
spec:
  type: NodePort
  ports:
  - name: ui
    port: 16686
    targetPort: 16686
    nodePort: 30686
  selector:
    app: jaeger 